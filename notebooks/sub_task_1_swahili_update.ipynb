{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "sub_task_1_swahili_update.ipynb",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "eYQvW0qFWy0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "# =============================================================================\n",
        "# 0. A: CHEMINS DES FICHIERS FOURNIS (MIS Ã€ JOUR POUR TÃ‚CHE 1)\n",
        "# =============================================================================\n",
        "EN_TRAIN_FILE_PATH = '/content/eng_train_1.csv' # MIS Ã€ JOUR\n",
        "SW_TRAIN_FILE_PATH = '/content/swa_train_1.csv' # MIS Ã€ JOUR\n",
        "EN_TEST_FILE_PATH = '/content/eng_test_1.csv'   # MIS Ã€ JOUR\n",
        "\n",
        "# Le fichier de soumission prÃ©cÃ©dent (si nÃ©cessaire, mais non utilisÃ© dans ce pipeline)\n",
        "PREVIOUS_SUBMISSION_PATH = '/content/pred_eng.csv'\n",
        "\n",
        "# =============================================================================\n",
        "# 0. B: CONFIGURATION ET HYPERPARAMÃˆTRES\n",
        "# =============================================================================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# CONFIGURATION DU MODÃˆLE REMBERT (Classification)\n",
        "CONFIG_MODEL = {\n",
        "    'model_name': 'google/rembert',\n",
        "    'max_length': 128,\n",
        "    'k_folds': 5,\n",
        "    'batch_size': 8,\n",
        "    'gradient_accumulation_steps': 2,\n",
        "    'learning_rate': 2e-5,\n",
        "    'num_epochs': 5,\n",
        "    'output_dir_base': './rembert-polarization-en-kfold-TASK1-AUGMENTED',\n",
        "    'seed': SEED\n",
        "}\n",
        "\n",
        "# CONFIGURATION DE L'AUGMENTATION (Traduction NLLB)\n",
        "CONFIG_AUG = {\n",
        "    'nllb_model': \"facebook/nllb-200-distilled-600M\",\n",
        "    'src_lang_sw': \"swh_Latn\",\n",
        "    'tgt_lang_en': \"eng_Latn\",\n",
        "    'batch_size': 32\n",
        "}\n",
        "\n",
        "# DÃ©finir le device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ–¥ï¸ Device: {device} | ðŸŽ¯ TÃ¢che: Polarisation Binaire (Macro F1)\")\n",
        "\n",
        "# =============================================================================\n",
        "# 0. C: CLASSES ET FONCTIONS DE BASE (InchangÃ©es)\n",
        "# =============================================================================\n",
        "# (DÃ©finitions de PolarizationDataset, CustomTrainer, compute_metrics sont supposÃ©es Ãªtre ici)\n",
        "# ..."
      ],
      "metadata": {
        "id": "iETvYRnJz17o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765477233434,
          "user_tz": -120,
          "elapsed": 609,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ca3e40e5-8554-4b3a-ddc7-d464887dc9fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ–¥ï¸ Device: cuda | ðŸŽ¯ TÃ¢che: Polarisation Binaire (Macro F1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. DATA AUGMENTATION (SW Polarized -> EN)\n",
        "# =============================================================================\n",
        "def augment_polarized_data_binary(swa_file, config):\n",
        "    print(\"\\n--- 1. DATA AUGMENTATION (SW Polarized -> EN) ---\")\n",
        "\n",
        "    df_swa = pd.read_csv(swa_file)\n",
        "    df_swa.columns = df_swa.columns.str.strip()\n",
        "    text_col = next((c for c in df_swa.columns if c.lower() in ['text', 'text']), df_swa.columns[1])\n",
        "\n",
        "    # **Filtrer UNIQUEMENT les exemples classÃ©s comme 'Polarized' (label=1)**\n",
        "    df_polarized = df_swa[df_swa['label'] == 1].copy()\n",
        "\n",
        "    # ... (le reste de la fonction de traduction est inchangÃ©) ...\n",
        "    # 2. Charger le modÃ¨le NLLB\n",
        "    tokenizer_nllb = AutoTokenizer.from_pretrained(config['nllb_model'])\n",
        "    model_nllb = AutoModelForSeq2SeqLM.from_pretrained(config['nllb_model']).to(device)\n",
        "\n",
        "    # 3. Traduction\n",
        "    texts = df_polarized[text_col].astype(str).tolist()\n",
        "    results = []\n",
        "    tokenizer_nllb.src_lang = config['src_lang_sw']\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), config['batch_size']), desc=\"Translating SW->EN\"):\n",
        "        batch_texts = texts[i : i + config['batch_size']]\n",
        "        inputs = tokenizer_nllb(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            translated_tokens = model_nllb.generate(**inputs, forced_bos_token_id=tokenizer_nllb.convert_tokens_to_ids(config['tgt_lang_en']), max_length=128)\n",
        "        decoded = tokenizer_nllb.batch_decode(translated_tokens, skip_special_tokens=True)\n",
        "        results.extend(decoded)\n",
        "\n",
        "    # 4. Nettoyage et crÃ©ation du DataFrame augmentÃ©\n",
        "    del model_nllb\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    df_aug = df_polarized[['label']].copy()\n",
        "    df_aug[text_col] = results\n",
        "\n",
        "    return df_aug\n",
        "\n",
        "# ExÃ©cution de l'augmentation et fusion\n",
        "df_en_original = pd.read_csv(EN_TRAIN_FILE_PATH)\n",
        "df_en_original.columns = df_en_original.columns.str.strip()\n",
        "\n",
        "df_aug_en = augment_polarized_data_binary(SW_TRAIN_FILE_PATH, CONFIG_AUG)\n",
        "\n",
        "if df_aug_en is not None:\n",
        "    # Assurer la cohÃ©rence des colonnes\n",
        "    text_col_orig = next((c for c in df_en_original.columns if c.lower() in ['text', 'text']), df_en_original.columns[1])\n",
        "    df_aug_en = df_aug_en.rename(columns={df_aug_en.columns[-1]: text_col_orig})\n",
        "    df_en_final = pd.concat([df_en_original, df_aug_en], ignore_index=True)\n",
        "else:\n",
        "    df_en_final = df_en_original\n",
        "\n",
        "print(f\"\\nâœ… Total Anglais (Original + Augmentation): {len(df_en_final):,} Ã©chantillons.\")\n",
        "print(f\"  Polarized (label=1) count: {df_en_final['label'].sum()}\")"
      ],
      "metadata": {
        "id": "QtcFqmQvnxRW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1765477245194,
          "user_tz": -120,
          "elapsed": 610,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d4d4322d-f2d8-43ed-8d99-b5ef3dc9a1ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. DATA AUGMENTATION (SW Polarized -> EN) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3716680709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mdf_en_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_en_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdf_aug_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_polarized_data_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSW_TRAIN_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG_AUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf_aug_en\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3716680709.py\u001b[0m in \u001b[0;36maugment_polarized_data_binary\u001b[0;34m(swa_file, config)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# **Filtrer UNIQUEMENT les exemples classÃ©s comme 'Polarized' (label=1)**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf_polarized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_swa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_swa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# ... (le reste de la fonction de traduction est inchangÃ©) ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. K-FOLD TRAINING SUR LE DATASET AUGMENTÃ‰ (RemBERT)\n",
        "# =============================================================================\n",
        "def run_k_fold_training_augmented(df_full, config):\n",
        "    # ... (Le corps de la fonction est inchangÃ© par rapport Ã  la rÃ©ponse prÃ©cÃ©dente) ...\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸš€ STARTING K-FOLD TRAINING on AUGMENTED DATA (RemBERT)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    texts_all = df_full['text'].tolist()\n",
        "    labels_all = df_full['label'].values\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=config['k_folds'], shuffle=True, random_state=config['seed'])\n",
        "    all_fold_metrics = []\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(texts_all, labels_all)):\n",
        "        # ... (Logique de Split, Poids de Classe, et chargement du ModÃ¨le RemBERT) ...\n",
        "        train_labels = labels_all[train_index]\n",
        "        weight_0 = len(train_labels) / (2.0 * np.sum(train_labels == 0))\n",
        "        weight_1 = len(train_labels) / (2.0 * np.sum(train_labels == 1))\n",
        "        class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32)\n",
        "\n",
        "        train_dataset = PolarizationDataset([texts_all[i] for i in train_index], [labels_all[i] for i in train_index], tokenizer, config['max_length'])\n",
        "        val_dataset = PolarizationDataset([texts_all[i] for i in val_index], [labels_all[i] for i in val_index], tokenizer, config['max_length'])\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=2)\n",
        "\n",
        "        output_dir_fold = f\"{config['output_dir_base']}/fold_{fold+1}\"\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir_fold, num_train_epochs=config['num_epochs'], per_device_train_batch_size=config['batch_size'],\n",
        "            per_device_eval_batch_size=config['batch_size'], learning_rate=config['learning_rate'], warmup_ratio=config['warmup_ratio'],\n",
        "            weight_decay=0.01, eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=1, load_best_model_at_end=True,\n",
        "            metric_for_best_model='macro_f1', greater_is_better=True, fp16=torch.cuda.is_available(),\n",
        "            gradient_accumulation_steps=config['gradient_accumulation_steps'], report_to=\"none\", seed=config['seed'],\n",
        "        )\n",
        "\n",
        "        trainer = CustomTrainer(\n",
        "            model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "            tokenizer=tokenizer, compute_metrics=compute_metrics, class_weights=class_weights,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        eval_results = trainer.evaluate()\n",
        "        print(f\"  Macro F1 (Fold {fold+1}): {eval_results['eval_macro_f1']:.4f}\")\n",
        "        trainer.save_model(output_dir_fold)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"K-FOLD TRAINING COMPLETED. Models saved.\")\n",
        "\n",
        "    # Note: Vous devriez vÃ©rifier ici si le F1 Macro est > 0.7566 !\n",
        "\n",
        "run_k_fold_training_augmented(df_en_final, CONFIG_MODEL)"
      ],
      "metadata": {
        "id": "EV9HaYFPonzB",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1765477110979,
          "user_tz": -120,
          "elapsed": 11,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 3. PRÃ‰DICTION FINALE (Vote d'Ensemble K-Fold)\n",
        "# =============================================================================\n",
        "def predict_ensemble_voting(test_file_path, model_path_base, k_folds, max_length):\n",
        "    print(\"\\n--- 3. PRÃ‰DICTION PAR VOTE D'ENSEMBLE ---\")\n",
        "\n",
        "    df_test = pd.read_csv(test_file_path)\n",
        "    df_test.columns = df_test.columns.str.strip()\n",
        "    id_col = next((c for c in df_test.columns if c.lower() in ['id', 'id']), df_test.columns[0])\n",
        "    text_col = next((c for c in df_test.columns if c.lower() in ['text', 'text']), df_test.columns[1])\n",
        "    test_texts = df_test[text_col].astype(str).tolist()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"{model_path_base}1\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    all_fold_predictions = np.zeros((len(test_texts), k_folds), dtype=int)\n",
        "\n",
        "    for fold in range(k_folds):\n",
        "        fold_model_path = f\"{model_path_base}{fold+1}\"\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(fold_model_path)\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "\n",
        "        fold_predictions = []\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(0, len(test_texts), 64), desc=f\"Predicting Fold {fold+1}\"):\n",
        "                batch_texts = test_texts[i:i + 64]\n",
        "                encodings = tokenizer(batch_texts, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt').to(device)\n",
        "                outputs = model(**encodings)\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "                fold_predictions.extend(predictions)\n",
        "\n",
        "        all_fold_predictions[:, fold] = np.array(fold_predictions)\n",
        "        del model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Vote Majoritaire\n",
        "    ensemble_votes = np.sum(all_fold_predictions, axis=1)\n",
        "    final_predictions = (ensemble_votes > (k_folds / 2)).astype(int)\n",
        "\n",
        "    # CrÃ©ation du fichier de soumission\n",
        "    submission_df = pd.DataFrame()\n",
        "    submission_df[id_col] = df_test[id_col]\n",
        "    submission_df['label'] = final_predictions\n",
        "\n",
        "    output_filename = 'polarization_english_augmented_submission.csv'\n",
        "    submission_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"\\nâœ… Fichier de soumission FINAL crÃ©Ã© : {output_filename}\")\n",
        "    return submission_df\n",
        "\n",
        "# ExÃ©cution de la prÃ©diction\n",
        "submission_task1 = predict_ensemble_voting(\n",
        "    EN_TEST_FILE_PATH,\n",
        "    CONFIG_MODEL['output_dir_base'] + '/fold_',\n",
        "    CONFIG_MODEL['k_folds'],\n",
        "    CONFIG_MODEL['max_length']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "2Fk-mbd6orxG",
        "executionInfo": {
          "status": "error",
          "timestamp": 1765477110979,
          "user_tz": -120,
          "elapsed": 11,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d97d353d-1e5c-41ef-abaa-d95165558b3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. PRÃ‰DICTION PAR VOTE D'ENSEMBLE ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './rembert-polarization-en-kfold-TASK1-AUGMENTED/fold_1'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './rembert-polarization-en-kfold-TASK1-AUGMENTED/fold_1'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2640156425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# ExÃ©cution de la prÃ©diction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m submission_task1 = predict_ensemble_voting(\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mEN_TEST_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mCONFIG_MODEL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir_base'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/fold_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2640156425.py\u001b[0m in \u001b[0;36mpredict_ensemble_voting\u001b[0;34m(test_file_path, model_path_base, k_folds, max_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_path_base}1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './rembert-polarization-en-kfold-TASK1-AUGMENTED/fold_1'. Use `repo_type` argument if needed."
          ]
        }
      ]
    }
  ]
}