{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "subtask2_eng.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch pandas scikit-learn transformers accelerate -U"
      ],
      "metadata": {
        "id": "9IFGu6sCApTr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895414459,
          "user_tz": -120,
          "elapsed": 582,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R√©installer PyTorch et transformers\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers tokenizers datasets accelerate\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚ö†Ô∏è  ATTENTION : VOUS DEVEZ MAINTENANT RED√âMARRER LE RUNTIME !\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüëâ Menu : Runtime > Restart runtime\")\n",
        "print(\"üëâ Ou utilisez le raccourci : Ctrl+M .\")\n",
        "print(\"\\nüö´ N'ex√©cutez AUCUNE autre cellule avant le red√©marrage !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fG2AeyouC3PS",
        "executionInfo": {
          "status": "error",
          "timestamp": 1765895538071,
          "user_tz": -120,
          "elapsed": 15179,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c728a494-c42a-47f5-c1af-67da0444dc69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.2/663.9 MB\u001b[0m \u001b[31m214.6 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m226.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3778624903.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# R√©installer PyTorch et transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers tokenizers datasets accelerate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell must be executed FIRST after restart\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda if torch.cuda.is_available() else \"N/A\")\n",
        "\n",
        "# Simple test\n",
        "try:\n",
        "    x = torch.randn(2, 2)\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    print(f\"Test successful - Device: {x.device}\")\n",
        "    print(\"\\nEverything works! You can now execute your code.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rhuLPdZBGH_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895545038,
          "user_tz": -120,
          "elapsed": 2977,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5ae3e4c0-cad0-4bf1-d9f4-4305499f2d0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Transformers version: 4.55.0\n",
            "CUDA available: True\n",
            "CUDA version: 12.4\n",
            "Test successful - Device: cuda:0\n",
            "\n",
            "Everything works! You can now execute your code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "5Z5LDp8jAhuH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895550856,
          "user_tz": -120,
          "elapsed": 3195,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version verification\n",
        "import torch\n",
        "import transformers\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Import test\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "print(\"Import successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "H7xYgpcG_2Ey",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895555043,
          "user_tz": -120,
          "elapsed": 4192,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9a4aedc5-6de7-444f-c9d8-265aeef346f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Transformers version: 4.55.0\n",
            "CUDA available: True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# --- Constants ---\n",
        "MODEL_NAME = \"google/rembert\"\n",
        "LABEL_COLUMNS = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
        "N_SPLITS = 5\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# --- Data Loading ---\n",
        "# CORRECTION 1: 'path' was not defined\n",
        "path = \"/content/eng_train_2.csv\"  # Replace with actual path\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# CORRECTION 2: Verify that columns exist\n",
        "print(f\"Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "# CORRECTION 3: Handle missing values before conversion\n",
        "df[LABEL_COLUMNS] = df[LABEL_COLUMNS].fillna(0).astype(float)\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Label examples:\\n{df[LABEL_COLUMNS].head()}\")\n",
        "\n",
        "# --- Additional Checks ---\n",
        "# CORRECTION 4: Verify that text column exists\n",
        "text_column = 'text'  # Adjust according to your dataset\n",
        "if text_column not in df.columns:\n",
        "    print(f\"ERROR: Column '{text_column}' does not exist!\")\n",
        "    print(f\"Available columns: {df.columns.tolist()}\")\n",
        "else:\n",
        "    print(f\"Text column found: {text_column}\")\n",
        "    print(f\"Text example: {df[text_column].iloc[0][:100]}...\")\n",
        "\n",
        "# CORRECTION 5: Check label distribution\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df[LABEL_COLUMNS].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4MJD49g_mLy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895555043,
          "user_tz": -120,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "db03a290-a545-43a3-e6b6-bd357fefd60f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns: ['id', 'text', 'political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
            "Dataset Shape: (3222, 7)\n",
            "Label examples:\n",
            "   political  racial/ethnic  religious  gender/sexual  other\n",
            "0        0.0            0.0        0.0            0.0    0.0\n",
            "1        0.0            0.0        0.0            0.0    0.0\n",
            "2        0.0            0.0        0.0            0.0    0.0\n",
            "3        0.0            0.0        0.0            0.0    0.0\n",
            "4        0.0            0.0        0.0            0.0    0.0\n",
            "Text column found: text\n",
            "Text example:  is defending imperialism in the dnd chat...\n",
            "\n",
            "Label distribution:\n",
            "political        1150.0\n",
            "racial/ethnic     281.0\n",
            "religious         112.0\n",
            "gender/sexual      72.0\n",
            "other             126.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ---------------------------------------------\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "# Make plots look nicer\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. READ THE DATA\n",
        "# ---------------------------------------------\n",
        "path = \"/content/eng_train_2.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "x2gil8oS79N3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895555647,
          "user_tz": -120,
          "elapsed": 608,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b36dc769-cd4b-4f74-f15d-ab0df93504b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (3222, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     id  \\\n",
              "0  eng_973938b90b0ff5d87d35a582f83f5c89   \n",
              "1  eng_07dfd4600426caca6e2c5883fcbea9ea   \n",
              "2  eng_f14519ff2302b6cd47712073f13bc461   \n",
              "3  eng_e48b7e7542faafa544ac57b64bc80daf   \n",
              "4  eng_7c581fb77bce8033aeba3d6dbd6273eb   \n",
              "\n",
              "                                                text  political  \\\n",
              "0           is defending imperialism in the dnd chat          0   \n",
              "1  Still playing with this. I am now following Ra...          0   \n",
              "2  .senate.gov Theres 3 groups out there Republic...          0   \n",
              "3  \"ABC MD, David Anderson, said the additional f...          0   \n",
              "4  \"bad people\" I have some conservative values s...          0   \n",
              "\n",
              "   racial/ethnic  religious  gender/sexual  other  \n",
              "0              0          0              0      0  \n",
              "1              0          0              0      0  \n",
              "2              0          0              0      0  \n",
              "3              0          0              0      0  \n",
              "4              0          0              0      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b43338c-d210-4eba-a1d4-af68dd4167e4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>political</th>\n",
              "      <th>racial/ethnic</th>\n",
              "      <th>religious</th>\n",
              "      <th>gender/sexual</th>\n",
              "      <th>other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eng_973938b90b0ff5d87d35a582f83f5c89</td>\n",
              "      <td>is defending imperialism in the dnd chat</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eng_07dfd4600426caca6e2c5883fcbea9ea</td>\n",
              "      <td>Still playing with this. I am now following Ra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eng_f14519ff2302b6cd47712073f13bc461</td>\n",
              "      <td>.senate.gov Theres 3 groups out there Republic...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>eng_e48b7e7542faafa544ac57b64bc80daf</td>\n",
              "      <td>\"ABC MD, David Anderson, said the additional f...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>eng_7c581fb77bce8033aeba3d6dbd6273eb</td>\n",
              "      <td>\"bad people\" I have some conservative values s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b43338c-d210-4eba-a1d4-af68dd4167e4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3b43338c-d210-4eba-a1d4-af68dd4167e4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3b43338c-d210-4eba-a1d4-af68dd4167e4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e5f0e29e-66ea-4f92-92d9-bfeed7e661c2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5f0e29e-66ea-4f92-92d9-bfeed7e661c2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e5f0e29e-66ea-4f92-92d9-bfeed7e661c2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3222,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3222,\n        \"samples\": [\n          \"eng_a77b7a8a526f324ea4e2c718bc602093\",\n          \"eng_5f27d15a507eab2a883a540606fb2033\",\n          \"eng_17bde4509f6827a979d61cbc2067927b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3222,\n        \"samples\": [\n          \"Donald Trump relies on First Amendment\",\n          \"House GOP in no rush to give more Ukraine aid after 6\",\n          \"Israeli adviser to meet with US officials on war\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"political\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"racial/ethnic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"religious\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender/sexual\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"other\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data and Defining Constants"
      ],
      "metadata": {
        "id": "OsLVuOqy8Bkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# --- Constants ---\n",
        "MODEL_NAME = \"google/rembert\"\n",
        "LABEL_COLUMNS = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
        "N_SPLITS = 5\n",
        "MAX_LENGTH = 64  # Maximum sequence length (adjust if necessary)\n",
        "\n",
        "# --- Data Loading ---\n",
        "path = \"/content/eng_train_2.csv\"  # Define the path variable\n",
        "df = pd.read_csv(path)  # Use the actual path to your file\n",
        "\n",
        "# Convert labels to float for BCEWithLogitsLoss\n",
        "df[LABEL_COLUMNS] = df[LABEL_COLUMNS].astype(float)\n",
        "print(f\"Dataset Shape: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH4ctm697c9U",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895555647,
          "user_tz": -120,
          "elapsed": 7,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "02093a12-a0b8-46c9-c064-91320b772eee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (3222, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the PyTorch Dataset"
      ],
      "metadata": {
        "id": "mc1ZbKGH8C2R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CU5mandW2_6Y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895557376,
          "user_tz": -120,
          "elapsed": 1173,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "class PolarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Initialisation du tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric (Macro F1)"
      ],
      "metadata": {
        "id": "wYkYmOJR8LE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_macro_f1(preds_logits, labels):\n",
        "    # Model predictions are logits\n",
        "    # For evaluation during training, use a default threshold (0.5)\n",
        "    preds = (torch.sigmoid(torch.from_numpy(preds_logits)).numpy() > 0.5).astype(int)\n",
        "\n",
        "    # Calculate Macro F1-score\n",
        "    macro_f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
        "\n",
        "    return {\"macro_f1\": macro_f1}\n",
        "\n",
        "# Function for Trainer (requires a function that takes an EvalPrediction object)\n",
        "def compute_metrics(p):\n",
        "    return compute_macro_f1(p.predictions, p.label_ids)"
      ],
      "metadata": {
        "id": "EnzjE1Ze8L8V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895557940,
          "user_tz": -120,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-Fold Cross-Validation and Logits Collection"
      ],
      "metadata": {
        "id": "q3Xs2NLs8N55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "Lk6URL_vD9IB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895559751,
          "user_tz": -120,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clear GPU Memory ---\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear all cached memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Reset peak memory stats\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "print(\"GPU memory cleared!\")\n",
        "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"Memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRXPaDm_kOnw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1765895561311,
          "user_tz": -120,
          "elapsed": 589,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ad77421d-f4b9-4ad6-c0b1-f32bd41c7a98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory cleared!\n",
            "Memory allocated: 0.00 GB\n",
            "Memory cached: 0.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# --- Constants ---\n",
        "MODEL_NAME = \"google/rembert\"\n",
        "LABEL_COLUMNS = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
        "MAX_LENGTH = 64\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- Load Data ---\n",
        "path = \"/content/eng_train_2.csv\"\n",
        "df = pd.read_csv(path)\n",
        "df[LABEL_COLUMNS] = df[LABEL_COLUMNS].astype(float)\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Compute Metrics Function ---\n",
        "def compute_macro_f1(preds_logits, labels):\n",
        "    # Model predictions are logits\n",
        "    # For evaluation during training, use a default threshold (0.5)\n",
        "    preds = (torch.sigmoid(torch.from_numpy(preds_logits)).numpy() > 0.5).astype(int)\n",
        "\n",
        "    # Calculate Binary F1-score\n",
        "    f1 = f1_score(labels, preds, average='binary', zero_division=0)\n",
        "\n",
        "    return {\"f1\": f1}\n",
        "\n",
        "def compute_metrics(p):\n",
        "    return compute_macro_f1(p.predictions, p.label_ids)\n",
        "\n",
        "# --- Dictionary to store trained models and predictions ---\n",
        "trained_models = {}\n",
        "val_predictions_per_class = {}\n",
        "val_labels_per_class = {}\n",
        "\n",
        "# --- Train a separate model for each polarization class ---\n",
        "for label_name in LABEL_COLUMNS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING MODEL FOR: {label_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. Split data into train and validation\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=df[label_name]\n",
        "    )\n",
        "\n",
        "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
        "    print(f\"Class distribution - Train: {train_df[label_name].value_counts().to_dict()}\")\n",
        "    print(f\"Class distribution - Val: {val_df[label_name].value_counts().to_dict()}\")\n",
        "\n",
        "    # 2. Create datasets with single label\n",
        "    train_dataset = PolarizationDataset(\n",
        "        train_df['text'].tolist(),\n",
        "        train_df[[label_name]].values,  # Single label column\n",
        "        tokenizer,\n",
        "        MAX_LENGTH\n",
        "    )\n",
        "    val_dataset = PolarizationDataset(\n",
        "        val_df['text'].tolist(),\n",
        "        val_df[[label_name]].values,  # Single label column\n",
        "        tokenizer,\n",
        "        MAX_LENGTH\n",
        "    )\n",
        "\n",
        "    # 3. Load Model (binary classification for this specific class)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=1,  # Binary classification: 1 output neuron\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    # 4. Configure training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_{label_name}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        per_device_eval_batch_size=4,\n",
        "        learning_rate=3e-5,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.1,\n",
        "        logging_dir=f'./logs_{label_name}',\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    # 5. Create Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # 6. Training\n",
        "    print(f\"\\nTraining {label_name} model...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # 7. Validation predictions\n",
        "    val_predictions = trainer.predict(val_dataset)\n",
        "\n",
        "    # Store predictions and labels\n",
        "    val_predictions_per_class[label_name] = val_predictions.predictions\n",
        "    val_labels_per_class[label_name] = val_predictions.label_ids\n",
        "\n",
        "    # Calculate F1 score for this class\n",
        "    preds = (torch.sigmoid(torch.from_numpy(val_predictions.predictions)).numpy() > 0.5).astype(int)\n",
        "    class_f1 = f1_score(val_predictions.label_ids, preds, average='binary', zero_division=0)\n",
        "    print(f\"\\n{label_name} - Validation F1-Score: {class_f1:.4f}\")\n",
        "\n",
        "    # 8. Save the trained model\n",
        "    trained_models[label_name] = trainer\n",
        "    model.save_pretrained(f'./model_{label_name}')\n",
        "\n",
        "    # 9. Free GPU memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n--- Completed training for {label_name} ---\")\n",
        "\n",
        "# --- Aggregate results across all classes ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Combine all predictions\n",
        "all_val_logits = np.concatenate([val_predictions_per_class[label] for label in LABEL_COLUMNS], axis=1)\n",
        "all_val_labels = np.concatenate([val_labels_per_class[label] for label in LABEL_COLUMNS], axis=1)\n",
        "\n",
        "# Calculate overall macro F1\n",
        "final_preds = (torch.sigmoid(torch.from_numpy(all_val_logits)).numpy() > 0.5).astype(int)\n",
        "macro_f1_overall = f1_score(all_val_labels, final_preds, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\nIndividual Class F1-Scores:\")\n",
        "for label_name in LABEL_COLUMNS:\n",
        "    preds = (torch.sigmoid(torch.from_numpy(val_predictions_per_class[label_name])).numpy() > 0.5).astype(int)\n",
        "    class_f1 = f1_score(val_labels_per_class[label_name], preds, average='binary', zero_division=0)\n",
        "    print(f\"  {label_name}: {class_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nOverall Macro F1-Score: {macro_f1_overall:.4f}\")\n",
        "print(\"\\nAll models saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7leS2-6MeaM0",
        "executionInfo": {
          "status": "error",
          "timestamp": 1765897009018,
          "user_tz": -120,
          "elapsed": 1447134,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e83f4603-e330-44b7-f051-d2119648f3d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (3222, 7)\n",
            "\n",
            "============================================================\n",
            "TRAINING MODEL FOR: POLITICAL\n",
            "============================================================\n",
            "Train size: 2577, Validation size: 645\n",
            "Class distribution - Train: {0.0: 1657, 1.0: 920}\n",
            "Class distribution - Val: {0.0: 415, 1.0: 230}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3863991236.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training political model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [486/486 23:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.589100</td>\n",
              "      <td>0.447204</td>\n",
              "      <td>0.724638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.442100</td>\n",
              "      <td>0.514095</td>\n",
              "      <td>0.515337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.401200</td>\n",
              "      <td>0.655388</td>\n",
              "      <td>0.647887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "political - Validation F1-Score: 0.7246\n",
            "\n",
            "--- Completed training for political ---\n",
            "\n",
            "============================================================\n",
            "TRAINING MODEL FOR: RACIAL/ETHNIC\n",
            "============================================================\n",
            "Train size: 2577, Validation size: 645\n",
            "Class distribution - Train: {0.0: 2352, 1.0: 225}\n",
            "Class distribution - Val: {0.0: 589, 1.0: 56}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3863991236.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training racial/ethnic model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 17.75 MiB is free. Process 29378 has 14.54 GiB memory in use. Of the allocated memory 12.73 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3863991236.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# 6. Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTraining {label_name} model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# 7. Validation predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_patched_step_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mpatched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpatched_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0maccelerated_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerate_step_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpatched_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 )\n\u001b[1;32m    170\u001b[0m                 \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 state[\"exp_avg\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 17.75 MiB is free. Process 29378 has 14.54 GiB memory in use. Of the allocated memory 12.73 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- Configuration ---\n",
        "N_SPLITS = 5\n",
        "LABEL_COLUMNS = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
        "\n",
        "# Dictionary to store OOF predictions for each class\n",
        "all_class_oof_logits = {label: [] for label in LABEL_COLUMNS}\n",
        "all_class_oof_labels = {label: [] for label in LABEL_COLUMNS}\n",
        "\n",
        "# --- Train a separate model for each polarization class ---\n",
        "for label_name in LABEL_COLUMNS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING MODEL FOR: {label_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(df)):\n",
        "        print(f\"\\n--- Starting Fold {fold+1}/{N_SPLITS} for {label_name} ---\")\n",
        "\n",
        "        # 1. Prepare fold data with single label\n",
        "        train_df = df.iloc[train_index]\n",
        "        val_df = df.iloc[val_index]\n",
        "\n",
        "        train_dataset = PolarizationDataset(\n",
        "            train_df['text'].tolist(),\n",
        "            train_df[[label_name]].values,  # Single label column\n",
        "            tokenizer,\n",
        "            MAX_LENGTH\n",
        "        )\n",
        "        val_dataset = PolarizationDataset(\n",
        "            val_df['text'].tolist(),\n",
        "            val_df[[label_name]].values,  # Single label column\n",
        "            tokenizer,\n",
        "            MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        # 2. Load Model (binary classification for this specific class)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=1,  # Binary classification: 1 output neuron\n",
        "            problem_type=\"multi_label_classification\"\n",
        "        )\n",
        "\n",
        "        # 3. Configure training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f'./results_{label_name}_fold_{fold}',\n",
        "            num_train_epochs=5,\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=8,\n",
        "            per_device_eval_batch_size=4,\n",
        "            learning_rate=3e-5,\n",
        "            warmup_steps=500,\n",
        "            weight_decay=0.1,\n",
        "            logging_dir=f'./logs_{label_name}',\n",
        "            logging_steps=100,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"macro_f1\",\n",
        "            greater_is_better=True,\n",
        "            fp16=True\n",
        "        )\n",
        "\n",
        "        # 4. Create Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        # 5. Training\n",
        "        print(f\"Training {label_name} model...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # 6. OOF Predictions on validation set\n",
        "        oof_predictions = trainer.predict(val_dataset)\n",
        "\n",
        "        # Store logits for this class\n",
        "        all_class_oof_logits[label_name].append(oof_predictions.predictions)\n",
        "        all_class_oof_labels[label_name].append(oof_predictions.label_ids)\n",
        "\n",
        "        # 7. Free GPU memory\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n--- Completed all folds for {label_name} ---\")\n",
        "\n",
        "# --- Aggregate OOF results for each class ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AGGREGATING OUT-OF-FOLD PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "oof_logits_per_class = {}\n",
        "oof_labels_per_class = {}\n",
        "\n",
        "for label_name in LABEL_COLUMNS:\n",
        "    oof_logits_per_class[label_name] = np.concatenate(all_class_oof_logits[label_name], axis=0)\n",
        "    oof_labels_per_class[label_name] = np.concatenate(all_class_oof_labels[label_name], axis=0)\n",
        "\n",
        "    # Calculate F1 score for this class\n",
        "    preds = (torch.sigmoid(torch.from_numpy(oof_logits_per_class[label_name])).numpy() > 0.5).astype(int)\n",
        "    class_f1 = f1_score(oof_labels_per_class[label_name], preds, average='binary', zero_division=0)\n",
        "    print(f\"{label_name}: F1-Score = {class_f1:.4f}\")\n",
        "\n",
        "# Combine all predictions for final macro F1 calculation\n",
        "all_oof_logits = np.concatenate([oof_logits_per_class[label] for label in LABEL_COLUMNS], axis=1)\n",
        "all_oof_labels = np.concatenate([oof_labels_per_class[label] for label in LABEL_COLUMNS], axis=1)\n",
        "\n",
        "# Calculate overall macro F1\n",
        "final_preds = (torch.sigmoid(torch.from_numpy(all_oof_logits)).numpy() > 0.5).astype(int)\n",
        "macro_f1_overall = f1_score(all_oof_labels, final_preds, average='macro', zero_division=0)\n",
        "print(f\"\\nOverall Macro F1-Score: {macro_f1_overall:.4f}\")"
      ],
      "metadata": {
        "id": "I8Zx3d-R8SSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimisation du Seuil par Classe (Le point cl√©)"
      ],
      "metadata": {
        "id": "4ZtM012I8ZVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# 3. LABEL DISTRIBUTION\n",
        "# ---------------------------------------------\n",
        "label_cols = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "label_counts = df[label_cols].sum().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
        "plt.title(\"Label Distribution\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uzv3bBAs62Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "for col in label_cols:\n",
        "    count = df[col].value_counts()\n",
        "    print(f\"{col}: {count}\")"
      ],
      "metadata": {
        "id": "ipdT0lUs9xhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "for col in label_cols:\n",
        "    count = df[col].sum()\n",
        "    print(f\"{col}: {count} rows\")\n"
      ],
      "metadata": {
        "id": "UCgGB5eR8rbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Define colors for each label type and slight variations for classes 0 and 1\n",
        "color_scheme = {\n",
        "    \"political\":       (\"#9ecae1\", \"#08519c\"),  # light blue, dark blue\n",
        "    \"racial/ethnic\":   (\"#a1d99b\", \"#238b45\"),  # light green, dark green\n",
        "    \"religious\":       (\"#cbc9e2\", \"#6a51a3\"),  # light purple, dark purple\n",
        "    \"gender/sexual\":   (\"#fcbba1\", \"#cb181d\"),  # light red, dark red\n",
        "    \"other\":           (\"#fdd0a2\", \"#e6550d\")   # light orange, dark orange\n",
        "}\n",
        "\n",
        "label_cols = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "for col in label_cols:\n",
        "    print(f\"Generating colored wordclouds for: {col}\")\n",
        "\n",
        "    # Fetch custom colors\n",
        "    color_0, color_1 = color_scheme[col]\n",
        "\n",
        "    # Create separate word lists\n",
        "    text_0 = \" \".join(df[df[col] == 0][\"text\"].astype(str))\n",
        "    text_1 = \" \".join(df[df[col] == 1][\"text\"].astype(str))\n",
        "\n",
        "    # Custom color functions\n",
        "    def color_func_0(*args, **kwargs):\n",
        "        return color_0\n",
        "\n",
        "    def color_func_1(*args, **kwargs):\n",
        "        return color_1\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 10), dpi=300)\n",
        "    fig.suptitle(f\"WordClouds for '{col}'\", fontsize=22)\n",
        "\n",
        "    # WordCloud for class 0\n",
        "    wc0 = WordCloud(width=1000, height=800, background_color='white',\n",
        "                    color_func=color_func_0).generate(text_0)\n",
        "    axes[0].imshow(wc0, interpolation='bilinear')\n",
        "    axes[0].set_title(f\"{col} = 0 (Non-class)\", fontsize=18)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # WordCloud for class 1\n",
        "    wc1 = WordCloud(width=1000, height=800, background_color='white',\n",
        "                    color_func=color_func_1).generate(text_1)\n",
        "    axes[1].imshow(wc1, interpolation='bilinear')\n",
        "    axes[1].set_title(f\"{col} = 1 (Class)\", fontsize=18)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8JgqcQih7zsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 1. Dataset\n",
        "# ============================================================\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[[\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]].values.astype(float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. Focal Loss with Class Weights\n",
        "# ============================================================\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # tensor of size [num_labels]\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce = F.binary_cross_entropy_with_logits(\n",
        "            logits, targets, reduction=\"none\"\n",
        "        )\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
        "\n",
        "        focal = (self.alpha * (1 - pt) ** self.gamma * bce)\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return focal.mean()\n",
        "        else:\n",
        "            return focal.sum()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. Load Data\n",
        "# ============================================================\n",
        "\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/subtask2/eng-train-task2.csv\")\n",
        "df = df.rename(columns={\"label\":\"other\"})  # only if needed\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Sizes -> train/val/test:\", len(train_df), len(val_df), len(test_df))\n",
        "\n",
        "# ============================================================\n",
        "# 4. Compute Class Weights\n",
        "# ============================================================\n",
        "\n",
        "label_cols = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "pos_counts = train_df[label_cols].sum(axis=0).values\n",
        "neg_counts = len(train_df) - pos_counts\n",
        "weights = torch.tensor(neg_counts / pos_counts, dtype=torch.float)\n",
        "\n",
        "print(\"Class weights:\", weights)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Load Model + Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=5,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ").to(device)\n",
        "\n",
        "# ============================================================\n",
        "# 6. Dataloaders\n",
        "# ============================================================\n",
        "\n",
        "train_ds = TextDataset(train_df, tokenizer)\n",
        "val_ds = TextDataset(val_df, tokenizer)\n",
        "test_ds = TextDataset(test_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "# ============================================================\n",
        "# 7. Loss + Optimizer\n",
        "# ============================================================\n",
        "\n",
        "criterion = FocalLoss(alpha=weights.to(device), gamma=2.0)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 4\n",
        "\n",
        "# ============================================================\n",
        "# 8. Train / Eval Functions\n",
        "# ============================================================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Train\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attn = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attn)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def eval_one_epoch(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Eval\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attn = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask=attn).logits\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            all_preds.append(probs)\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "\n",
        "    macro_f1 = f1_score(all_labels, (all_preds > 0.5), average=\"macro\")\n",
        "\n",
        "    return total_loss / len(loader), macro_f1, all_preds, all_labels\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 9. Training Loop\n",
        "# ============================================================\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
        "    val_loss, val_f1, _, _ = eval_one_epoch(model, val_loader)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Macro F1: {val_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 10. Threshold Search on Val Set\n",
        "# ============================================================\n",
        "\n",
        "def find_best_thresholds(preds, labels):\n",
        "    thresholds = {}\n",
        "    for i, col in enumerate(label_cols):\n",
        "        best_f1 = 0\n",
        "        best_t = 0.5\n",
        "        for t in np.linspace(0.05, 0.95, 20):\n",
        "            f1 = f1_score(labels[:, i], (preds[:, i] > t))\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_t = t\n",
        "        thresholds[col] = np.float64(best_t)\n",
        "    return thresholds\n",
        "\n",
        "_, _, val_preds, val_labels = eval_one_epoch(model, val_loader)\n",
        "thresholds = find_best_thresholds(val_preds, val_labels)\n",
        "print(\"Best thresholds:\", thresholds)\n",
        "\n",
        "# ============================================================\n",
        "# 11. FINAL TEST EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "def apply_thresholds(preds, thresholds):\n",
        "    binarized = np.zeros_like(preds)\n",
        "    for i, col in enumerate(label_cols):\n",
        "        binarized[:, i] = preds[:, i] > thresholds[col]\n",
        "    return binarized\n",
        "\n",
        "_, _, test_preds, test_labels = eval_one_epoch(model, test_loader)\n",
        "bin_preds = apply_thresholds(test_preds, thresholds)\n",
        "\n",
        "macro_f1 = f1_score(test_labels, bin_preds, average=\"macro\")\n",
        "micro_f1 = f1_score(test_labels, bin_preds, average=\"micro\")\n",
        "\n",
        "print(\"\\n=== FINAL TEST SCORES ===\")\n",
        "print(\"Macro F1:\", macro_f1)\n",
        "print(\"Micro F1:\", micro_f1)\n"
      ],
      "metadata": {
        "id": "g3SJ45-wfQzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "### 1. LOAD THE DEV SET (THE ONE YOU MUST SUBMIT)\n",
        "df_dev = pd.read_csv(\"/content/gdrive/MyDrive/subtask2/eng-dev-task2.csv\")\n",
        "\n",
        "# If your dataset column is not exactly \"text\", modify here:\n",
        "texts = df_dev[\"text\"].tolist()\n",
        "\n",
        "### 2. DATASET FOR PREDICTION\n",
        "class PredDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len=256):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "        }\n",
        "\n",
        "pred_dataset = PredDataset(texts, tokenizer)\n",
        "pred_loader = DataLoader(pred_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "### 3. GET RAW PREDICTIONS\n",
        "all_logits = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(pred_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        all_logits.extend(probs.cpu().numpy())\n",
        "\n",
        "all_logits = np.array(all_logits)\n",
        "\n",
        "### 4. APPLY BEST THRESHOLDS YOU FOUND\n",
        "thresholds = {\n",
        "    \"political\": 0.381578947368421,\n",
        "    \"racial/ethnic\": 0.33421052631578946,\n",
        "    \"religious\": 0.381578947368421,\n",
        "    \"gender/sexual\": 0.381578947368421,\n",
        "    \"other\": 0.14473684210526316\n",
        "}\n",
        "\n",
        "label_order = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "preds_binary = np.zeros_like(all_logits)\n",
        "\n",
        "for i, label in enumerate(label_order):\n",
        "    preds_binary[:, i] = (all_logits[:, i] >= thresholds[label]).astype(int)\n",
        "\n",
        "### 5. CREATE SUBMISSION FILE\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": df_dev[\"id\"],\n",
        "    \"political\": preds_binary[:, 0],\n",
        "    \"racial/ethnic\": preds_binary[:, 1],\n",
        "    \"religious\": preds_binary[:, 2],\n",
        "    \"gender/sexual\": preds_binary[:, 3],\n",
        "    \"other\": preds_binary[:, 4],\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Saved submission.csv!\")\n"
      ],
      "metadata": {
        "id": "wkx0S0AKj4_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "########################################\n",
        "# 1. LOAD TRAIN + DEV DATASETS\n",
        "########################################\n",
        "\n",
        "train_df = pd.read_csv(\"/content/gdrive/MyDrive/subtask2/eng-train-task2.csv\")\n",
        "dev_df   = pd.read_csv(\"/content/gdrive/MyDrive/subtask2/eng-dev-task2.csv\")\n",
        "\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COLS = [\"political\",\"racial/ethnic\",\"religious\",\"gender/sexual\",\"other\"]\n",
        "NUM_LABELS = len(LABEL_COLS)\n",
        "\n",
        "########################################\n",
        "# 2. STRATIFY TARGET FOR MULTILABEL\n",
        "########################################\n",
        "\n",
        "# Convert multilabel ‚Üí string for stratification (best practice)\n",
        "train_df[\"stratify_label\"] = train_df[LABEL_COLS].astype(str).agg(\"\".join, axis=1)\n",
        "\n",
        "########################################\n",
        "# 3. Dataset Class\n",
        "########################################\n",
        "\n",
        "class HateDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.texts = df[TEXT_COL].tolist()\n",
        "        self.labels = df[LABEL_COLS].values\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "########################################\n",
        "# 4. Model + Loss Function\n",
        "########################################\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
        "\n",
        "def create_model():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"cardiffnlp/twitter-roberta-base-hate-latest\",\n",
        "        num_labels=NUM_LABELS,\n",
        "        problem_type=\"multi_label_classification\",\n",
        "        ignore_mismatched_sizes=True # Added this line to fix the size mismatch error\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "########################################\n",
        "# 5. Train One Fold\n",
        "########################################\n",
        "\n",
        "def train_fold(model, train_loader, val_loader, epochs=3):\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
        "            optimizer.zero_grad()\n",
        "            batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = criterion(outputs.logits, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(f\"Train Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "    # Validation predictions\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            labels = batch[\"labels\"].numpy()\n",
        "            batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits.sigmoid().cpu().numpy()\n",
        "\n",
        "            preds.append(logits)\n",
        "            trues.append(labels)\n",
        "\n",
        "    preds = np.vstack(preds)\n",
        "    trues = np.vstack(trues)\n",
        "\n",
        "    # threshold = 0.3 (best for Cardiff hate model)\n",
        "    bin_preds = (preds > 0.3).astype(int)\n",
        "\n",
        "    f1 = f1_score(trues, bin_preds, average=\"micro\")\n",
        "    print(\"Validation Micro F1:\", f1)\n",
        "\n",
        "    return model, preds, f1\n",
        "\n",
        "########################################\n",
        "# 6. K-Fold Cross Validation\n",
        "########################################\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_preds = []\n",
        "fold_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df[\"stratify_label\"])):\n",
        "    print(f\"\\n===== FOLD {fold+1} =====\")\n",
        "    model = create_model()\n",
        "\n",
        "    train_data = train_df.iloc[train_idx]\n",
        "    val_data   = train_df.iloc[val_idx]\n",
        "\n",
        "    train_ds = HateDataset(train_data, tokenizer)\n",
        "    val_ds   = HateDataset(val_data, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "    model, preds, f1 = train_fold(model, train_loader, val_loader)\n",
        "    fold_preds.append(preds)\n",
        "    fold_scores.append(f1)\n",
        "\n",
        "print(\"\\n=== MEAN CV SCORE:\", np.mean(fold_scores), \"===\")\n",
        "\n",
        "########################################\n",
        "# 7. Train FINAL MODEL on ALL TRAIN DATA\n",
        "########################################\n",
        "\n",
        "final_model = create_model()\n",
        "full_ds = HateDataset(train_df, tokenizer)\n",
        "full_loader = DataLoader(full_ds, batch_size=16, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(final_model.parameters(), lr=2e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    final_model.train()\n",
        "    losses = []\n",
        "    for batch in tqdm(full_loader, desc=f\"Final Train Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        batch = {k:v.to(DEVICE) for k,v in batch.items()}\n",
        "        outputs = final_model(**batch)\n",
        "        loss = criterion(outputs.logits, batch[\"labels\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    print(\"Final Train Loss:\", np.mean(losses))\n",
        "\n",
        "########################################\n",
        "# 8. INFERENCE ON DEV\n",
        "########################################\n",
        "\n",
        "dev_ds = HateDataset(dev_df, tokenizer)\n",
        "dev_loader = DataLoader(dev_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "final_model.eval()\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dev_loader):\n",
        "        batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
        "        logits = final_model(**batch).logits.sigmoid().cpu().numpy()\n",
        "        all_preds.append(logits)\n",
        "\n",
        "all_preds = np.vstack(all_preds)\n",
        "binary_preds = (all_preds > 0.3).astype(int)\n",
        "\n",
        "########################################\n",
        "# 9. SAVE SUBMISSION\n",
        "########################################\n",
        "\n",
        "out = dev_df.copy()\n",
        "for i, col in enumerate(LABEL_COLS):\n",
        "    out[col] = binary_preds[:, i]\n",
        "\n",
        "out.to_csv(\"polar_submission.csv\", index=False)\n",
        "print(\"Saved ‚Üí polar_submission.csv\")"
      ],
      "metadata": {
        "id": "ROKyY8F7f7eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- PREDICT ON DEV + SAVE SUBMISSION ----------\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 2) label order (must match model output order)\n",
        "label_cols = [\"political\", \"racial/ethnic\", \"religious\", \"gender/sexual\", \"other\"]\n",
        "\n",
        "# 3) load dev file (path where you uploaded it)\n",
        "dev_path = \"/content/gdrive/MyDrive/subtask2/eng-dev-task2.csv\"   # adjust if different\n",
        "dev_df = pd.read_csv(dev_path)\n",
        "print(\"Dev loaded:\", dev_df.shape)\n",
        "\n",
        "# ensure id and text exist; try common alternatives\n",
        "if \"id\" not in dev_df.columns:\n",
        "    # try first column as id\n",
        "    dev_df = dev_df.rename(columns={dev_df.columns[0]: \"id\"})\n",
        "if \"text\" not in dev_df.columns:\n",
        "    # try common alternative names\n",
        "    for candidate in [\"tweet\", \"content\", \"message\"]:\n",
        "        if candidate in dev_df.columns:\n",
        "            dev_df = dev_df.rename(columns={candidate: \"text\"})\n",
        "            break\n",
        "# final fallback: convert second column to text if no 'text'\n",
        "if \"text\" not in dev_df.columns:\n",
        "    dev_df = dev_df.rename(columns={dev_df.columns[1]: \"text\"})\n",
        "\n",
        "# ensure text is string\n",
        "dev_df[\"text\"] = dev_df[\"text\"].fillna(\"\").astype(str)\n",
        "\n",
        "# 4) load tokenizer + model (use in-memory if available, otherwise from disk)\n",
        "# Try to use existing variables if they exist in the notebook:\n",
        "try:\n",
        "    tokenizer  # noqa: F821\n",
        "    print(\"Using tokenizer from environment\")\n",
        "except Exception:\n",
        "    # fallback: try common saved tokenizer names\n",
        "    for path in [\"./final_roberta_multilabel\", \"./final_model\", \"./cardiff_final\", \"./final\"]:\n",
        "        if os.path.isdir(path):\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "                print(\"Loaded tokenizer from\", path)\n",
        "                break\n",
        "            except Exception:\n",
        "                tokenizer = None\n",
        "    if \"tokenizer\" not in globals() or tokenizer is None:\n",
        "        # last resort: load cardiff tokenizer (fast)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
        "        print(\"Loaded tokenizer from HuggingFace cardiffnlp/twitter-roberta-base-hate-latest\")\n",
        "\n",
        "try:\n",
        "    final_model  # noqa: F821\n",
        "    model = final_model\n",
        "    print(\"Using final_model from environment\")\n",
        "except Exception:\n",
        "    # try loading from common save locations\n",
        "    model = None\n",
        "    for path in [\"./final_roberta_multilabel\", \"./final_model\", \"./cardiff_final\", \"./final\"]:\n",
        "        if os.path.isdir(path):\n",
        "            try:\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=len(label_cols), problem_type=\"multi_label_classification\")\n",
        "                print(\"Loaded model from\", path)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(\"Could not load model from\", path, \":\", e)\n",
        "    if model is None:\n",
        "        # fallback to the Cardiff checkpoint and re-init classification head\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\",\n",
        "                                                                   num_labels=len(label_cols),\n",
        "                                                                   problem_type=\"multi_label_classification\")\n",
        "        print(\"Loaded cardiffnlp model and reinitialized head\")\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 5) Dataset + DataLoader for dev\n",
        "class DevDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        txt = self.texts[idx]\n",
        "        enc = self.tokenizer(txt,\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             max_length=self.max_len,\n",
        "                             return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "dev_dataset = DevDataset(dev_df[\"text\"].tolist(), tokenizer, max_len=128)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 6) thresholds: use existing 'thresholds' if present, else fallback to reasonable defaults\n",
        "try:\n",
        "    thresholds  # noqa: F821\n",
        "    print(\"Using thresholds from environment\")\n",
        "except Exception:\n",
        "    # fallback thresholds you reported earlier (adjust if you prefer)\n",
        "    thresholds = {\n",
        "        \"political\": 0.381578947368421,\n",
        "        \"racial/ethnic\": 0.33421052631578946,\n",
        "        \"religious\": 0.381578947368421,\n",
        "        \"gender/sexual\": 0.381578947368421,\n",
        "        \"other\": 0.14473684210526316\n",
        "    }\n",
        "    print(\"Using fallback thresholds:\", thresholds)\n",
        "\n",
        "# align thresholds into array in label_cols order\n",
        "th_arr = np.array([thresholds[c] for c in label_cols], dtype=float)\n",
        "\n",
        "# 7) predict probabilities\n",
        "all_probs = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dev_loader, desc=\"Predict\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = out.logits\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        all_probs.append(probs)\n",
        "all_probs = np.vstack(all_probs)  # shape (N_dev, 5)\n",
        "print(\"Preds shape:\", all_probs.shape)\n",
        "\n",
        "# 8) binarize according to thresholds\n",
        "bin_preds = (all_probs > th_arr).astype(int)\n",
        "\n",
        "# 9) create submission dataframe with exact required columns\n",
        "sub_df = pd.DataFrame()\n",
        "sub_df[\"id\"] = dev_df[\"id\"].astype(str).values  # ensure id preserved as string\n",
        "for i, col in enumerate(label_cols):\n",
        "    sub_df[col] = bin_preds[:, i].astype(int)\n",
        "\n",
        "# 10) save CSV (no extra index or weird nested columns)\n",
        "out_path = \"./polar_submissiion.csv\"\n",
        "sub_df.to_csv(out_path, index=False)\n",
        "print(\"Saved submission to\", out_path)\n",
        "print(sub_df.head(5))\n"
      ],
      "metadata": {
        "id": "aQ672MzTn2vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subtask 3: Manifestation Identification"
      ],
      "metadata": {
        "id": "cpjykdQZdKWf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g33cdj2UqWFo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}